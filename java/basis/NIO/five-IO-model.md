---
title: 五种IO模型
mathjax: true
data: 2020-10-28 22:17:45
updated:
tags: IO模型
categories: IO
---

## 前言

相信对于很多新人来说，同步、异步、阻塞、与非阻塞这四个概念非常容易混淆。那是因为我们没有搞清楚我们看待问题的视角。对于这四个概念，站在不同的角度，如何区别是不同的。我在此将所有的资料汇总，总结，并提出我自己的见解。

我将分两种维度讨论这四种概念，分别是广义上的同步与阻塞，以及狭义上的同步与阻塞。

## 1. 广义维度下的区分

相信很多人在编写多线程程序最大的难度就是让各个线程之间同步合作。那么根据<<操作系统概念>>(第九版)一书中关于进程之间通信部分小节的同步异步概念:([怎样理解阻塞非阻塞与同步异步的区别?](https://www.zhihu.com/question/19732473))
![os](os.jpg)

其中说到进程之间的消息传递可以分为阻塞或者非阻塞的，也即是同步或者异步的。发送和接受动作分别可以细分为两种情况：

- 阻塞发送：发送进程会被阻塞直到消息被接受
- 非阻塞发送：发送进程发送完毕后可以进行其他操作
- 阻塞接受：接收进程会被阻塞直到消息可用
- 非阻塞接受：接收进程收到消息或返回空

那么站在我们多线程程序的角度，我们可以认为同步即阻塞，异步即非阻塞。无论底层是如何实现的。

## 2. 狭义维度下的区分

所谓的狭义维度即我们进行IO操作的角度。因为IO操作的特殊性，阻塞是可能发生的。那么下面我将通过介绍五种linux平台的IO模型来区分这四种概念：

- 阻塞IO模型
- 非阻塞IO模型
- IO多路复用模型
- 信号通知模型
- 异步IO模型

首先我们肯定是要通过内核与外部设备进行IO交互，那么一次IO操作基本上可以分为三个步骤：

1. 用户进程等待IO设备的数据
2. 通知用户进程所需数据已经准备好
3. 用户进程把IO数据从内核空间拷贝至用户空间

了解这三个步骤后，很自然地会想到第一部分的等待是如何等待，是占用CPU空等还是进行睡眠操作？第二部分的通知什么时候通知？（我们上面的IO操作基于单线程网络IO）下面的四种模型很好地回答了上述这两个问题。

### 2.1 阻塞IO模型

首先阻塞IO模型最符合我们的惯性思维，模型如下图所示(图片应该来自unix网络编程一书，未经本人考证):

![blocking-IO](blocking-IO.jpg)

在linux的世界里，一切皆文件。那么我们也可以将socket当作一种特殊的文件来进行独写。在默认情况下socket为**阻塞**模式，那么当用户进程调用`recvfrom`后，由于此时IO数据尚未准备完成，用户进程会被阻塞(处于等待阶段)，直到数据准备完成。此时用户进程还是会被阻塞至数据拷贝至用户空间（处于拷贝阶段）。

可以看到，在一次IO操作中用户线程是全程被阻塞的，所以这是最基本的阻塞IO模型。

### 2.2 非阻塞IO模型

![non-blocking-IO](non-blocking-IO.jpg)

上图为非阻塞IO模型的基本示意图。

在进行网络IO时，我们可以将socket设置为**非阻塞**模式。这个设置就是告诉内核对于这个socket，调用`recvfrom`时如果数据尚未准备完成，那么直接返回，不必等待至数据准备好。所以在第一阶段我们不必等待。

那么什么时候用户进程会得知内核完成数据准备呢(即第二阶段)?在这个模型中，用户进程会不断进行轮循操作，即不断调用`recvfrom`来得知数据是否准备完毕，从而完成第二个阶段。

第三个阶段与阻塞IO模型类型，都是从内核空间将数据拷贝出来。可以看到第二阶段的轮循操作是非常浪费资源的，因为大多时候我们在做无用功。从而产生了IO多路复用模型。

### 2.3 IO多路复用模型

所谓的**多路**是指多个socket的读写，**复用**是指所有的IO都可以通过复用一个或几个线程来完成。基本模型如下图所示：

![IO-multixing](multiplexing-IO.jpg)

IO多路复用模型的工作场景如下：

假设我们现在有socket A、B需要进行IO操作，有三种方案：

1. 单线程，采用阻塞IO模型：这就是将所有的IO操作串行，当两个socket流量都非常低且不活跃时，效率非常低
2. 单线程，采用非阻塞IO模型，不断对A、B进行轮循操作，当然仍然会浪费大量的时间
3. 采用多线程，阻塞IO模型，将A、B的IO轮循任务分配两个子线程。虽然这样效率比前面两种方案都高，但是线程是非常宝贵的资源，创建与销毁线程代价昂贵

难道我们就不能让socket完成数据准备后自己通知用户线程吗？这样就能避免大量不必要的轮循操作。当然这是可行的。也就是使用linux平台的select、poll、epoll等系统调用。对应于java中的NIO。

这里我首先以`select`作为例子。基本的流程如下：

1. 我们将需要操作的socket注册到`select`函数中，并绑定我们感兴趣的操作，例如是读还是写。然后调用`select`。**注意`select`仍然会阻塞用户进程**。
2. `select`会不断的对注册的socket进行轮循操作，直至有可用的socket出现，此时该函数会返回，但剩余未准备好的socket仍然可以继续准备。
3. 对可用的socket进行感兴趣的操作。然后继续调用`select`

那么我们节省的时间在哪里？很简单，就是在当我们select出一些socket后进行数据处理操作时，剩余的socket仍然可以继续准备。可能当我们下一次调用`select`时又有新的socket已经准备好了。这样就避免对多个socket进行轮循时，已经准备好的socket后轮循，没准备好的先轮循，浪费了不必要的时间。

当然对于`select`,还有更多的细节需要注意。这里只需要记住，对于第一个等待数据的阶段同样会**产生阻塞**，拷贝数据时也会**产生阻塞**。


### 2.4 信号通知模型


### 2.5 异步IO模型



### select

缺点：

1. 在**每次**调用`select`时都会将目标socket集合`fd_set`从用户空间拷贝至内核空间，所以当socket集合很大时，每次拷贝的效率会非常低
2. `select`内部每次使用**轮询操作**探查是否有socket的对应事件准备完毕，因为其使用的`fd_set`由数组组成，大小一般限制为1024(因为操作系统对每个进程可用的最大描述符数限制了上限，可在编译时重新设置)，所以每次轮询都需要完整遍历，这又会使`select`效率变低
3. 在找到准备好的socket集合后，`select`又会将所有的`fd_set`再从内核拷贝至用户空间，再次将`select`效率变低
4. 在每次调用`select`时，之前设置的`fd_set`都会失效，所以每次循环前都需要重新设置`fd_set`.


### poll

`poll`与`select`并无大的差别，只不过`poll`使用的socket集合`pollfd`没有大小的限制，因为底层是采用链表实现的，所以`select`有的缺点`poll`都会存在

### epoll

`epoll`是`poll`的升级版，`epoll`的API分为三个部分：

``` c
int epoll_create(int size);
int epoll_ctl(int epfd, int op, int fd, struct epoll_event *event);
int epoll_wait(int epfd, struct epoll_event * events, int maxevents, int timeout);
```

1. epoll_create：首先调用`epoll_create`创建`poll`对象。并且会开辟一个红黑树与就绪队列。红黑树用来保存我们需要监听的socket结合，就绪队列用来保存已经准备就绪的socket集合
2. epoll_ctl：注册要监听的事件类型。在每次注册新的事件到epoll句柄中时，会把对应的socket复制到内核中，注意对于一个socket，在`epoll`中只会被复制一次而不像`select`每次调用时都会复制。并且同时会向内核注册回调函数，大致功能是当该socket事件完成时将其加入就绪队列
3. epoll_wait：等待事件的就绪，其只用遍历就绪队列，所以`epoll`的复杂度至于活跃的连接数有关。并且就绪socket集合采用了内存映射，减少了拷贝fds的操作。同时`epoll_wait`返回后，会将就绪socket对应事件清空，如果后续仍想关注当前处理的socket，那么就需要用epoll_ctl(epfd,EPOLL_CTL_MOD,listenfd,&ev)来重新设置socket fd的事件类型，**而不需要重新注册fd**

epoll解决了什么问题：

1. epoll通过回调机制实现了知道哪个socket的哪些事件准备完成，这在`select`中需要通过轮询完成(这里并不是指在`epoll_wait`返回后不需要遍历小于返回值的fd，仍然需要循环遍历socket进行处理)
2. epoll只会在fd初次注册使用`epoll_ctl`时拷贝至内核,而`select`每次都需要完整的拷贝所有fd，并且每次都需要重新设置描述符结合，因为每次`select`返回后都会修改描述符集合

### 小结


select低效的原因之一是将“维护等待队列”和“阻塞进程”两个步骤合二为一。如下图所示，每次调用select都需要这两步操作，然而大多数应用场景中，需要监视的socket相对固定，并不需要每次都修改。epoll将这两个操作分开，先用epoll_ctl维护等待队列，再调用epoll_wait阻塞进程。显而易见的，效率就能得到提升。



## 参考文献

1. [同步I/O(阻塞I/O，非阻塞I/O)，异步I/O](https://cllc.fun/2019/03/07/synchronous-asynchronous-io/)

2. [Java sockets I/O: blocking, non-blocking and asynchronous](https://medium.com/@liakh.aliaksandr/java-sockets-i-o-blocking-non-blocking-and-asynchronous-fb7f066e4ede)

3. [POSIX的一些相关定义](https://pubs.opengroup.org/onlinepubs/9699919799/basedefs/V1_chap03.html)

4. [怎样理解阻塞非阻塞与同步异步的区别？](https://www.zhihu.com/question/19732473)

5. [I/O 多路复用，select / poll / epoll 详解](https://imageslr.github.io/2020/02/27/select-poll-epoll.html)

6. [Linux编程之select](https://www.cnblogs.com/skyfsm/p/7079458.html)